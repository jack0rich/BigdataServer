import httpx
from typing import Dict, Any, List, Optional
from app.core.config import settings
from app.utils.logger import logger


class HadoopAPIClient:
    """Hadoop REST API å®¢æˆ·ç«¯å°è£…"""

    def __init__(self):
        self.base_url = f"http://{settings.HADOOP_HOST}:{settings.HADOOP_PORT}/webhdfs/v1"
        self.client = httpx.AsyncClient()
        self.common_params = {"user.name": settings.HADOOP_USER}

    async def check_connection(self):
        """æ£€æŸ¥ WebHDFS è¿æ¥æ˜¯å¦å¯ç”¨"""
        try:
            params = {**self.common_params, "op": "GETHOMEDIRECTORY"}
            logger.info(f"ğŸ” æµ‹è¯• Hadoop è¿æ¥: {self.base_url}")
            resp = await self.client.get(self.base_url, params=params)
            resp.raise_for_status()
            logger.info(f"âœ… Hadoop WebHDFS è¿æ¥æˆåŠŸ: {resp.json()}")
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ HTTP é”™è¯¯: {e.response.status_code} - {e.response.text}")
            raise
        except httpx.RequestError as e:
            logger.error(f"âŒ æ— æ³•è¿æ¥åˆ° Hadoop: {e}")
            raise

    class HDFSNotFoundError(Exception):
        """è·¯å¾„ä¸å­˜åœ¨å¼‚å¸¸"""
        pass

    class HDFSConflictError(Exception):
        """è·¯å¾„å†²çªå¼‚å¸¸"""
        pass

    async def upload_file(
            self,
            hdfs_path: str,
            file_content: bytes,
            overwrite: bool = False,
            blocksize: int = 134217728,  # é»˜è®¤ 128MB
            replication: int = 3,
            permission: str = "755",
            buffersize: int = 4096,
            noredirect: bool = False
    ) -> Dict[str, Any]:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ° HDFS
        - overwrite: æ˜¯å¦è¦†ç›–å·²æœ‰æ–‡ä»¶
        - blocksize: HDFS å—å¤§å°
        - replication: å‰¯æœ¬æ•°
        - permission: æ–‡ä»¶æƒé™
        - buffersize: ç¼“å†²åŒºå¤§å°
        - noredirect: æ˜¯å¦ä¸è‡ªåŠ¨é‡å®šå‘
        """
        params = {
            "op": "CREATE",
            "overwrite": str(overwrite).lower(),
            "blocksize": str(blocksize),
            "replication": str(replication),
            "permission": permission,
            "buffersize": str(buffersize),
            "noredirect": str(noredirect).lower(),
            "user.name": settings.HADOOP_USER  # è¿™é‡Œ NameNode éœ€è¦ user.name
        }
        try:
            print(f"åˆ›å»ºæ–‡ä»¶è¯·æ±‚: {self.base_url}{hdfs_path}?{params}")

            # **ç¬¬ä¸€æ­¥**: å‘é€ CREATE è¯·æ±‚ï¼Œè·å– DataNode çš„è·³è½¬åœ°å€
            create_resp = await self.client.put(
                f"{self.base_url}{hdfs_path}",
                params=params,
                follow_redirects=False
            )

            if create_resp.status_code == 307:
                upload_url = create_resp.headers.get("Location")
                if not upload_url:
                    raise Exception("Hadoop æ²¡æœ‰è¿”å›æœ‰æ•ˆçš„ä¸Šä¼  URL")

                print(f"ä¸Šä¼  URL: {upload_url}")

                # **ç¬¬äºŒæ­¥**: å‘é€ PUT è¯·æ±‚ï¼Œå°†æ•°æ®ä¸Šä¼ åˆ° DataNode (âŒ è¿™é‡Œä¸è¦å†å¸¦ `user.name`!)
                upload_resp = await self.client.put(
                    upload_url,
                    content=file_content,
                    headers={"Content-Type": "application/octet-stream"}
                )
                upload_resp.raise_for_status()
            else:
                create_resp.raise_for_status()

            # **ç¬¬ä¸‰æ­¥**: ç¡®è®¤æ–‡ä»¶æ˜¯å¦ä¸Šä¼ æˆåŠŸ
            return await self.get_file_status(hdfs_path)

        except httpx.HTTPStatusError as e:
            print(f"HTTP é”™è¯¯: {e.response.status_code}, {e.response.text}")
            raise

    async def download_file(self, hdfs_path: str) -> bytes:
        """ä» HDFS ä¸‹è½½æ–‡ä»¶"""
        params = {**self.common_params, "op": "OPEN"}
        try:
            resp = await self.client.get(f"{self.base_url}{hdfs_path}", params=params, follow_redirects=True)
            resp.raise_for_status()
            return resp.content
        except httpx.HTTPStatusError as e:
            await self._handle_http_error(e)

    async def delete_path(self, hdfs_path: str, recursive: bool = False) -> None:
        """åˆ é™¤ HDFS è·¯å¾„"""
        params = {**self.common_params, "op": "DELETE", "recursive": str(recursive).lower()}
        try:
            resp = await self.client.delete(f"{self.base_url}{hdfs_path}", params=params)
            resp.raise_for_status()
        except httpx.HTTPStatusError as e:
            await self._handle_http_error(e)

    async def mkdir(self, hdfs_path: str, permission: Optional[str] = None) -> None:
        """åˆ›å»º HDFS ç›®å½•"""
        params = {**self.common_params, "op": "MKDIRS"}
        if permission:
            params["permission"] = permission

        try:
            resp = await self.client.put(f"{self.base_url}{hdfs_path}", params=params)
            resp.raise_for_status()
        except httpx.HTTPStatusError as e:
            await self._handle_http_error(e)

    async def rename_path(self, src_path: str, dest_path: str) -> None:
        """é‡å‘½åæˆ–ç§»åŠ¨ HDFS è·¯å¾„"""
        params = {**self.common_params, "op": "RENAME", "destination": dest_path}
        try:
            resp = await self.client.put(f"{self.base_url}{src_path}", params=params)
            resp.raise_for_status()
        except httpx.HTTPStatusError as e:
            await self._handle_http_error(e)

    async def list_dir(self, hdfs_path: str) -> List[Dict[str, Any]]:
        """åˆ—å‡º HDFS ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å’Œå­ç›®å½•"""
        params = {"op": "LISTSTATUS", "user.name": settings.HADOOP_USER}
        try:
            resp = await self.client.get(f"{self.base_url}{hdfs_path}", params=params)
            resp.raise_for_status()
            file_list = resp.json()["FileStatuses"]["FileStatus"]
            return [{"path": f"{hdfs_path}/{f['pathSuffix']}", "type": f['type'], "size": f["length"]} for f in file_list]
        except httpx.HTTPStatusError as e:
            await self._handle_http_error(e)

    async def get_file_status(self, hdfs_path: str) -> Dict[str, Any]:
        """è·å– HDFS æ–‡ä»¶çŠ¶æ€"""
        params = {"op": "GETFILESTATUS", "user.name": settings.HADOOP_USER}
        try:
            resp = await self.client.get(f"{self.base_url}{hdfs_path}", params=params)
            resp.raise_for_status()
            return self._parse_file_status(resp.json()["FileStatus"], hdfs_path)
        except httpx.HTTPStatusError as e:
            await self._handle_http_error(e)

    def _parse_file_status(self, status_data: Dict[str, Any], hdfs_path: str) -> Dict[str, Any]:
        """è§£æ HDFS æ–‡ä»¶çŠ¶æ€å“åº”"""
        return {
            "hdfs_path": hdfs_path,
            "file_size": status_data["length"],
            "block_size": status_data["blockSize"],
            "replication": status_data["replication"],
            "type": status_data["type"]
        }

    async def _handle_http_error(self, error: httpx.HTTPStatusError):
        """ç»Ÿä¸€å¤„ç† HTTP é”™è¯¯"""
        logger.error(f"Hadoop API Error: {error}")

        try:
            error_msg = error.response.json().get("RemoteException", {}).get("message", "Unknown error")
        except Exception:
            error_msg = error.response.text  # å…œåº•å¤„ç†

        if error.response.status_code == 404:
            raise self.HDFSNotFoundError(f"Requested path not found: {error_msg}")
        elif error.response.status_code == 409:
            raise self.HDFSConflictError(f"Path already exists: {error_msg}")
        else:
            raise Exception(f"Hadoop API Error: {error_msg}")

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc, tb):
        await self.client.aclose()